{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "pd.options.display.max_columns = 60\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import forest, RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score, precision_recall_fscore_support\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "from scipy import linalg\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('train_set.csv')\n",
    "test_raw = pd.read_csv('test_set.csv')\n",
    "#descr = pd.read_csv('features_description.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw.shape, test_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_raw['type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying removing rows with only NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_row = train_raw.dropna(axis = 0,thresh = 2).copy()\n",
    "test_no_row = test_raw.dropna(axis = 0,thresh = 2).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_raw.shape[0] - train_no_row.shape[0],'rows are removed from train dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_raw.shape[0] - test_no_row.shape[0],'rows are removed from test dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Removed from train dataset by type:','\\n','\\n',\n",
    "      train_raw['type'].value_counts() - train_no_row['type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Removed from test dataset by type:','\\n','\\n',\n",
    "      test_raw['type'].value_counts() - test_no_row['type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_row['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_no_row['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data (df):\n",
    "    \n",
    "    #Firstly, clean the data.\n",
    "    isna_cols = [c for c in df.columns if df[c].isna().sum() == len(df)]\n",
    "    df.drop(columns = isna_cols, inplace = True)\n",
    "    df.drop(columns = 'date_1d', inplace = True)# only 1 unique date (2017-02-05)\n",
    "    df.drop(columns = 'pct_data_consumption_eea', inplace = True)#corr 1.0 with 21 (pct_data_consumption_1d)\n",
    "    df.drop(columns = 'sli_data_1d', inplace = True)#corr almost 1.0 with 24 (sli_1d)\n",
    "    df.drop(columns = 'sli_data_eea', inplace = True)#corr almost 1.0 with 65 (sli_eea)\n",
    "    df.drop(columns = 'sli_weight_Data_1d', inplace = True)#corr almost 1.0 with 79 (sli_weight_1d)\n",
    "    df.drop(columns = 'sli_cause_data_1d', inplace = True)#duplicate of 25 (sli_cause_1d)\n",
    "    df.drop(columns = 'sli_cause_top_data_eea', inplace = True)#duplicate of 54 (sli_cause_top_eea)\n",
    "    df.drop(columns = 'sli_cause_detail_type_1d', inplace = True)#duplicate of 25 (sli_cause_1d)\n",
    "    df.drop(columns = 'sli_cause_detail_type_top_eea', inplace = True)#duplicate of 34 (sli_cause_top_eea)\n",
    "    df.drop(columns = 'sli_cause_detail_type_data_1d', inplace = True)#duplicate of 25 (sli_cause_1d)\n",
    "    df.drop(columns = 'sli_cause_detail_type_top_data_eea', inplace = True)#duplicate of 54 (sli_cause_top_eea)\n",
    "    df.drop(columns = 'file_sharing_count_dy_avg_eea', inplace = True)#corr almost 1.0 with 81 (sli_weight_MMS_1d)\n",
    "    \n",
    "    #counters\n",
    "    is_counts = ['audio_count_1d', 'audio_count_dy_avg_eea',\n",
    "                 'email_count_1d', 'email_count_dy_avg_eea',\n",
    "                 'file_sharing_count_1d', 'file_sharing_count_dy_avg_eea',\n",
    "                 'gaming_count_1d', 'gaming_count_dy_avg_eea',\n",
    "                 'im_count_1d', 'im_count_dy_avg_eea',\n",
    "                 'social_count_1d', 'social_count_dy_avg_eea',\n",
    "                 'video_count_1d', 'video_count_dy_avg_eea',\n",
    "                 'web_count_1d', 'web_count_dy_avg_eea']\n",
    "    \n",
    "    \n",
    "    #Secondly, manage categorial values\n",
    "    cat_hml = ('arpu_grp_eea', 'audio_count_grp_eea',\n",
    "               'email_count_grp_eea', 'file_sharing_count_grp_eea',\n",
    "               'gaming_count_grp_eea', 'im_count_grp_eea',\n",
    "               'social_count_grp_eea', 'video_count_grp_eea',\n",
    "               'web_count_grp_eea', 'sli_grp_eea') # [Low, Medium, High]\n",
    "    \n",
    "    cat_49_80_100 = ('pct_data_consumption_grp_eea', ) # [0-49, 80-100]\n",
    "    \n",
    "    cat_data_mms = ('sli_neg_impact_svc_1d', 'sli_neg_impact_svc_top_eea') # [data, MMS]\n",
    "    \n",
    "    cat_cic = ('sli_cause_1d', 'sli_cause_top_eea') # [Coverage, Internet, Congestion]\n",
    "    \n",
    "    cat_target = ('type', ) # [Mobile phone, Machine] - the target variable\n",
    "    \n",
    "    cat_dict = {cat_hml:['Low', 'Medium', 'High'],\n",
    "                cat_49_80_100:['0-49', '80-100'],\n",
    "                cat_data_mms:['data', 'MMS'],\n",
    "                cat_cic:['Coverage', 'Internet', 'Congestion'],\n",
    "                cat_target:['Mobile phone', 'Machine']\n",
    "               }\n",
    "    \n",
    "    for col, cat in cat_dict.items():\n",
    "        for c in col:\n",
    "            df[c] = pd.Categorical(df[c].values, categories=cat)\n",
    "            \n",
    "    # Thirdly, fix NAs\n",
    "    # counters - fill with 0, other with median\n",
    "    # categorical - replace with its code and increment by 1 (to avoid negative values)\n",
    "    for name, ser in df.copy().items():\n",
    "        if (ser.dtype == 'float64') and (name not in is_counts):\n",
    "            if pd.isna(ser).sum():\n",
    "                #df[name+'_na'] = pd.isnull(ser).astype('float')# mark values that filled with median\n",
    "                df[name] = ser.fillna(ser.median())\n",
    "                \n",
    "        elif (ser.dtype == 'float64') and (name in is_counts):\n",
    "            if pd.isna(ser).sum():\n",
    "                #df[name+'_na'] = pd.isnull(ser).astype('float')# mark values that filled with zeros\n",
    "                df[name] = ser.fillna(0)\n",
    "                \n",
    "        elif ser.dtype != 'bool':\n",
    "            if pd.isna(ser).sum():\n",
    "                df[name] = ser.cat.codes + 1\n",
    "    \n",
    "    features = [c for c in df.columns if c != 'type']\n",
    "    features_cat = cat_hml + cat_49_80_100 + cat_data_mms + cat_cic\n",
    "    \n",
    "    return df[features].values, df['type'].values.codes, features, features_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, features, features_cat= preprocess_data(train_raw.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y, _, _ = preprocess_data(test_raw.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest with all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_batch(y, batch_size = 2, weight_of_true_labels = 0.5, seed = 213):\n",
    "    \"\"\"Gets indices for a batch.\"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    true_labels_qty = int(batch_size * weight_of_true_labels)\n",
    "    false_labels_qty = batch_size - true_labels_qty\n",
    "    batch_inds = np.append(np.random.choice((np.where(y==0))[0], false_labels_qty, replace=True),\n",
    "                           np.random.choice((np.where(y==1))[0], true_labels_qty, replace=True))\n",
    "    return batch_inds\n",
    "\n",
    "def set_rf_batches(y, batch_size = 2, weight_of_true_labels = 0.5):\n",
    "    forest._generate_sample_indices = (\n",
    "        lambda rs, n_samples: set_batch(y=y,\n",
    "                                        batch_size = batch_size,\n",
    "                                        weight_of_true_labels = weight_of_true_labels,\n",
    "                                        seed = rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators':300,\n",
    "          'criterion':'gini', #entropy, gini\n",
    "          'max_depth':None,\n",
    "          'min_samples_split':2,\n",
    "          'min_samples_leaf':1,\n",
    "          'min_weight_fraction_leaf':0.0,\n",
    "          'max_features':0.5,\n",
    "          'max_leaf_nodes':None,\n",
    "          'min_impurity_decrease':0.0,\n",
    "          'min_impurity_split':None,\n",
    "          'bootstrap':True,\n",
    "          'oob_score':False,\n",
    "          'n_jobs':-1,\n",
    "          'random_state':None,\n",
    "          'verbose':0,\n",
    "          'warm_start':False,\n",
    "          'class_weight':None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_rf_batches(train_y, batch_size = 40000, weight_of_true_labels = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.stack([t.predict_proba(test_x)[:,1] for t in rf.estimators_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([roc_auc_score(test_y, np.mean(preds[:i+1], axis = 0)) for i in range(len(preds))]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (25,5))\n",
    "plt.margins(x=0.01, y=0.1)\n",
    "plt.plot(rf.feature_importances_, 'bo')\n",
    "plt.xticks(np.arange(train_x.shape[1]), labels = features, fontsize = 'x-small', rotation = 90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(model, t_x, v_x, t_y, v_y):\n",
    "    train_pred = model.predict(t_x)\n",
    "    test_pred = model.predict(v_x)\n",
    "    \n",
    "    print('train auc:', roc_auc_score(t_y, model.predict_proba(t_x)[:,1]))\n",
    "    print('test auc:', roc_auc_score(v_y, model.predict_proba(v_x)[:,1]))\n",
    "    print(\"-\"*15)\n",
    "    print('train accuracy:', (t_y == train_pred).sum() / len(t_y))\n",
    "    print('test accuracy:', (v_y == test_pred).sum()/ len(v_y))\n",
    "    print(\"-\"*15)\n",
    "    print('train f1 score:', f1_score(t_y, train_pred))\n",
    "    print('test f1 score:', f1_score(v_y, test_pred))\n",
    "    print(\"-\"*15)\n",
    "    print('train confusion matrix:','\\n', confusion_matrix(t_y, train_pred, [1,0]).T)\n",
    "    print('test confusion matrix:','\\n', confusion_matrix(v_y, test_pred, [1,0]).T)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_metrics(rf, train_x, test_x, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_machine_data(train_rows_x,train_rows_y,nrof_samples=5000,nrof_singular_values=3):\n",
    "    machine_idx = (train_rows_y==1)\n",
    "    train_rows_machine = train_rows_x[machine_idx]\n",
    "    \n",
    "    U,singular_values, Vh = linalg.svd(train_rows_machine.T)\n",
    "    singular_values = singular_values/np.max(singular_values)\n",
    "    weighted_mat = np.matmul(np.diag(singular_values[0:nrof_singular_values]),np.random.randn(\n",
    "        nrof_singular_values,nrof_samples))\n",
    "    new_machine_data = np.dot(U[:,0:nrof_singular_values],weighted_mat)\n",
    "    \n",
    "    new_machine_data_label = np.ones((nrof_samples,))\n",
    "    # Append the machine data to the training set\n",
    "    train_rows_x_resampled = np.append(train_rows_x,new_machine_data.T,axis=0)\n",
    "    \n",
    "    # Append the corresponding lables\n",
    "    train_rows_y_resampled = np.append(train_rows_y,new_machine_data_label,axis=0)\n",
    "    \n",
    "    return train_rows_x_resampled, train_rows_y_resampled, singular_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest with removed rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rows_x, train_rows_y, _, _ = preprocess_data(train_no_row.copy())\n",
    "test_rows_x, test_rows_y, _, _ = preprocess_data(test_no_row.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rows_x_resam, train_rows_y_resam, _= generate_new_machine_data(train_rows_x, train_rows_y, nrof_samples=10000,nrof_singular_values=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rows_x_resam.shape, train_rows_y_resam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_rf_batches(train_rows_y_resam, batch_size = 20000, weight_of_true_labels = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_na_rows = RandomForestClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "rf_na_rows.fit(train_rows_x_resam, train_rows_y_resam)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_no_rows = np.stack([t.predict_proba(test_rows_x)[:,1] for t in rf_na_rows.estimators_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot([roc_auc_score(test_rows_y, np.mean(preds_no_rows[:i+1], axis = 0)) for i in range(len(preds_no_rows))])\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('AUC');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (25,5))\n",
    "plt.margins(x=0.01, y=0.1)\n",
    "plt.plot(rf_na_rows.feature_importances_, 'bo')\n",
    "plt.xticks(np.arange(train_rows_x_resam.shape[1]), labels = features, fontsize = 'x-small', rotation = 90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_metrics(rf_na_rows, train_rows_x_resam, test_rows_x, train_rows_y_resam, test_rows_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fscore_support(test_rows_y, rf_na_rows.predict(test_rows_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_regr (df):\n",
    "    \n",
    "    #Firstly, clean the data.\n",
    "    isna_cols = [c for c in df.columns if df[c].isna().sum() == len(df)]\n",
    "    df.drop(columns = isna_cols, inplace = True)\n",
    "    df.drop(columns = 'date_1d', inplace = True)# only 1 unique date (2017-02-05)\n",
    "    df.drop(columns = 'pct_data_consumption_eea', inplace = True)#corr 1.0 with 21 (pct_data_consumption_1d)\n",
    "    df.drop(columns = 'sli_data_1d', inplace = True)#corr almost 1.0 with 24 (sli_1d)\n",
    "    df.drop(columns = 'sli_data_eea', inplace = True)#corr almost 1.0 with 65 (sli_eea)\n",
    "    df.drop(columns = 'sli_weight_Data_1d', inplace = True)#corr almost 1.0 with 79 (sli_weight_1d)\n",
    "    df.drop(columns = 'sli_cause_data_1d', inplace = True)#duplicate of 25 (sli_cause_1d)\n",
    "    df.drop(columns = 'sli_cause_top_data_eea', inplace = True)#duplicate of 54 (sli_cause_top_eea)\n",
    "    df.drop(columns = 'sli_cause_detail_type_1d', inplace = True)#duplicate of 25 (sli_cause_1d)\n",
    "    df.drop(columns = 'sli_cause_detail_type_top_eea', inplace = True)#duplicate of 34 (sli_cause_top_eea)\n",
    "    df.drop(columns = 'sli_cause_detail_type_data_1d', inplace = True)#duplicate of 25 (sli_cause_1d)\n",
    "    df.drop(columns = 'sli_cause_detail_type_top_data_eea', inplace = True)#duplicate of 54 (sli_cause_top_eea)\n",
    "    df.drop(columns = 'file_sharing_count_dy_avg_eea', inplace = True)#corr almost 1.0 with 81 (sli_weight_MMS_1d)\n",
    "    \n",
    "    #counters\n",
    "    is_counts = ('audio_count_1d', 'audio_count_dy_avg_eea',\n",
    "                 'email_count_1d', 'email_count_dy_avg_eea',\n",
    "                 'file_sharing_count_1d',\n",
    "                 'gaming_count_1d', 'gaming_count_dy_avg_eea',\n",
    "                 'im_count_1d', 'im_count_dy_avg_eea',\n",
    "                 'social_count_1d', 'social_count_dy_avg_eea',\n",
    "                 'video_count_1d', 'video_count_dy_avg_eea',\n",
    "                 'web_count_1d', 'web_count_dy_avg_eea')\n",
    "    \n",
    "    \n",
    "    #Secondly, manage categorial values\n",
    "    cat_hml = ('arpu_grp_eea', 'audio_count_grp_eea',\n",
    "               'email_count_grp_eea', 'file_sharing_count_grp_eea',\n",
    "               'gaming_count_grp_eea', 'im_count_grp_eea',\n",
    "               'social_count_grp_eea', 'video_count_grp_eea',\n",
    "               'web_count_grp_eea', 'sli_grp_eea') # [Low, Medium, High]\n",
    "    \n",
    "    cat_49_80_100 = ('pct_data_consumption_grp_eea', ) # [0-49, 80-100]\n",
    "    \n",
    "    cat_data_mms = ('sli_neg_impact_svc_1d', 'sli_neg_impact_svc_top_eea') # [data, MMS]\n",
    "    \n",
    "    cat_cic = ('sli_cause_1d', 'sli_cause_top_eea') # [Coverage, Internet, Congestion]\n",
    "    \n",
    "    cat_target = ('type', ) # [Mobile phone, Machine] - the target variable\n",
    "    \n",
    "    cat_dict = {cat_hml:['Low', 'Medium', 'High'],\n",
    "                cat_49_80_100:['0-49', '80-100'],\n",
    "                cat_data_mms:['data', 'MMS'],\n",
    "                cat_cic:['Coverage', 'Internet', 'Congestion'],\n",
    "                cat_target:['Mobile phone', 'Machine']\n",
    "               }\n",
    "\n",
    "    \n",
    "    features_one_hot = list(cat_hml + cat_49_80_100 + cat_data_mms + cat_cic + is_counts)\n",
    "    df = pd.get_dummies(df, prefix=None, prefix_sep='_', dummy_na=True,\n",
    "                   columns=features_one_hot, sparse=False, drop_first=False, dtype=None)\n",
    "    df['type'] = pd.Categorical(df['type'].values, categories=cat_target)\n",
    "    \n",
    "    features = [c for c in df.columns if c != 'type']\n",
    "    \n",
    "    return df[features].values, df['type'].values, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_regr, train_y_regr, _ = preprocess_data_regr(train_raw.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_regr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MinMaxScaler().fit(train_x_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_minmax = mm.transform(train_x_oh)\n",
    "test_x_minmax = mm.transform(train_x_oh)\n",
    "train_x_minmax.shape, test_x_minmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_mm = LogisticRegression(penalty='l2', #‘newton-cg’, ‘sag’ and ‘lbfgs’ for l2. ‘liblinear’, ‘saga’ for l1.\n",
    "                          dual=False,\n",
    "                          tol=0.00001,\n",
    "                          C=0.1,\n",
    "                          fit_intercept=True,\n",
    "                          intercept_scaling=1,\n",
    "                          class_weight=None, #'balanced', None\n",
    "                          random_state=None,\n",
    "                          solver='lbfgs', #‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’\n",
    "                          max_iter=3000, #For the newton-cg, sag and lbfgs solvers\n",
    "                          multi_class='ovr',\n",
    "                          verbose=0,\n",
    "                          warm_start=False,\n",
    "                          n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "logr_mm.fit(train_x_minmax, train_y)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_metrics(logr_mm, train_x_minmax, test_x_minmax, train_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression with removed rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_rows = MinMaxScaler().fit(train_rows_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rows_x_minmax = mm_rows.transform(train_rows_x)\n",
    "test_rows_x_minmax = mm_rows.transform(test_rows_x)\n",
    "train_rows_x_minmax.shape, test_rows_x_minmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_mm_rows = LogisticRegression(penalty='l2', #‘newton-cg’, ‘sag’ and ‘lbfgs’ for l2. ‘liblinear’, ‘saga’ for l1.\n",
    "                          dual=False,\n",
    "                          tol=0.00001,\n",
    "                          C=0.1,\n",
    "                          fit_intercept=True,\n",
    "                          intercept_scaling=1,\n",
    "                          class_weight=None, #'balanced', None\n",
    "                          random_state=None,\n",
    "                          solver='lbfgs', #‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’\n",
    "                          max_iter=3000, #For the newton-cg, sag and lbfgs solvers\n",
    "                          multi_class='ovr',\n",
    "                          verbose=0,\n",
    "                          warm_start=False,\n",
    "                          n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "logr_mm_rows.fit(train_rows_x_minmax, train_rows_y)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(logr_mm_rows, train_rows_x_minmax, test_rows_x_minmax, train_rows_y, test_rows_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(train_rows_x_minmax, train_rows_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_knn(model, t_x, v_x, t_y, v_y):\n",
    "    \n",
    "    train_pred = model.predict(t_x)\n",
    "    test_pred = model.predict(v_x)\n",
    "\n",
    "    print('train accuracy:', (t_y == train_pred).sum() / len(t_y))\n",
    "    print('test accuracy:', (v_y == test_pred).sum()/ len(v_y))\n",
    "    print(\"-\"*15)\n",
    "    print('train f1 score:', f1_score(t_y, train_pred))\n",
    "    print('test f1 score:', f1_score(v_y, test_pred))\n",
    "    print(\"-\"*15)\n",
    "    print('train confusion matrix:','\\n', confusion_matrix(t_y, train_pred, [1,0]).T)\n",
    "    print('test confusion matrix:','\\n', confusion_matrix(v_y, test_pred, [1,0]).T)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print_metrics_knn(knn, train_rows_x_minmax, test_rows_x_minmax, train_rows_y, test_rows_y)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(train_rows_x_resam, label = train_rows_y_resam, feature_name=features, categorical_feature='auto')\n",
    "test_data = lgb.Dataset(test_rows_x, label = test_rows_y, feature_name=features, categorical_feature='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_param = {'num_leaves':31,\n",
    "            'objective':'binary',\n",
    "            'is_unbalance':True,\n",
    "            'num_iterations':100,\n",
    "            'learning_rate':0.05,\n",
    "            'feature_fraction':0.5,\n",
    "            'early_stopping_round':10,\n",
    "            'verbosity':1}\n",
    "lm_param['metric'] = 'auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lm = lgb.train(lm_param, train_data, valid_sets=test_data)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_lgb(model, t_x, v_x, t_y, v_y):\n",
    "    \n",
    "    train_pred = model.predict(t_x, num_iteration=lm.best_iteration) > 0.5\n",
    "    test_pred = model.predict(v_x, num_iteration=lm.best_iteration) > 0.5\n",
    "\n",
    "    print('train accuracy:', (t_y == train_pred).sum() / len(t_y))\n",
    "    print('test accuracy:', (v_y == test_pred).sum()/ len(v_y))\n",
    "    print(\"-\"*15)\n",
    "    print('train f1 score:', f1_score(t_y, train_pred))\n",
    "    print('test f1 score:', f1_score(v_y, test_pred))\n",
    "    print(\"-\"*15)\n",
    "    print('train confusion matrix:','\\n', confusion_matrix(t_y, train_pred, [1,0]).T)\n",
    "    print('test confusion matrix:','\\n', confusion_matrix(v_y, test_pred, [1,0]).T)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics_lgb(lm, train_rows_x_resam, test_rows_x, train_rows_y_resam, test_rows_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_stack = rf_na_rows.predict_proba(test_rows_x)\n",
    "lm_stack = lm.predict(test_rows_x)\n",
    "log_mm_stack = logr_mm_rows.predict_proba(test_rows_x_minmax)\n",
    "\n",
    "print('rf_stack auc:', roc_auc_score(test_rows_y, rf_stack[:,1]))\n",
    "print('lr_stack auc:', roc_auc_score(test_rows_y, log_mm_stack[:,1]))\n",
    "print('lgb_stack auc:', roc_auc_score(test_rows_y, lm_stack))\n",
    "print('-'*15)\n",
    "print('average rf+lr auc:', roc_auc_score(test_rows_y, (rf_stack[:,1] + log_mm_stack[:,1]) / 2))\n",
    "print('average rf+lgb auc:', roc_auc_score(test_rows_y, (rf_stack[:,1] + lm_stack) / 2))\n",
    "print('average lr+lgb auc:', roc_auc_score(test_rows_y, (log_mm_stack[:,1] + lm_stack) / 2))\n",
    "print('-'*15)\n",
    "print('average rf+lr+lgb auc:', roc_auc_score(test_rows_y, (rf_stack[:,1] + log_mm_stack[:,1] + lm_stack) / 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_rows_y, ((rf_stack[:,1] + log_mm_stack[:,1]) / 2) > 0.5, [1,0]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
